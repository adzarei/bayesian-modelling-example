{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hierarchical Bayesian Linear Model with Experiment-Specific Offsets (TFP)\n",
    "\n",
    "**Author:** _<Your Name>_  \n",
    "**Course / PEC:** Hierarchical modeling with suspected zero-point offsets  \n",
    "**Dataset:** `properMotions.csv` (columns: `lab, x, y, sigma`)\n",
    "\n",
    "This notebook implements a hierarchical Bayesian linear model to infer the common linear relationship between \\(x\\) and \\(y\\) across \\(N\\) laboratories, and to test whether **experiment-dependent zero-point offsets** are supported by the data.\n",
    "\n",
    "**What you'll find here:**\n",
    "1. **Textual description** of the model, including **priors** and their **justification** (with references to *BDA3* and *Bayes Rules!*).  \n",
    "2. **Model evaluation**: posterior predictive checks and PSIS-LOO model comparison.  \n",
    "3. **Brief summary** template to report results once you run the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Model and priors (with justification)\n",
    "\n",
    "We have observations \\((x_{ij}, y_{ij})\\) from lab \\(i \\in \\{1,\\ldots, N\\}\\) and measurement \\(j \\in \\{1,\\ldots, n_i\\}\\). The measurement noise is Gaussian with **known** lab-specific standard deviation \\(\\sigma_i\\). Scientists suspect each lab may have a **zero-point calibration offset** \\(o_i\\). We compare two nested models.\n",
    "\n",
    "### Model A (baseline, no offsets)\n",
    "\\[\n",
    "y_{ij} \\sim \\mathcal{N}\\!\\big(m\\,x_{ij} + b,\\ \\sigma_i\\big).\n",
    "\\]\n",
    "\n",
    "### Model B (hierarchical offsets; partial pooling)\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "y_{ij} &\\sim \\mathcal{N}\\!\\big(m\\,x_{ij} + b + o_i,\\ \\sigma_i\\big),\\\\\n",
    "o_i \\mid \\tau &\\sim \\mathcal{N}(0,\\ \\tau),\\quad \\tau>0.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "Offsets \\(o_i\\) are **exchangeable** across labs, enabling **shrinkage** toward 0 when the data are weak—classic partial pooling (BDA3 **Ch. 5.1–5.7**).\n",
    "\n",
    "### Standardization\n",
    "For stable geometry with HMC/NUTS, we standardize:\n",
    "\\(\n",
    "x^{\\star}=(x-\\bar x)/s_x,\\quad y^{\\star}=(y-\\bar y)/s_y,\\quad \\sigma^{\\star}=\\sigma/s_y.\n",
    "\\)\n",
    "We fit the model on the standardized scale and then transform back to original units.\n",
    "\n",
    "### Priors (weakly informative, on standardized scale)\n",
    "\\[\n",
    "m^{\\star}\\sim\\mathcal{N}(0,1),\\qquad\n",
    "b^{\\star}\\sim\\mathcal{N}(0,1),\\qquad\n",
    "\\tau^{\\star}\\sim \\text{Half-Normal}(1),\\qquad\n",
    "o_i^{\\star} \\mid \\tau^{\\star} \\sim \\mathcal{N}(0,\\ \\tau^{\\star}).\n",
    "\\]\n",
    "These encode that typical slopes/intercepts are within a few SDs a priori, and that offsets are centered at 0 with an unknown scale allowing the data to decide the extent of lab-specific bias. See BDA3 **Ch. 14–15** (regression & hierarchical linear models) and **Ch. 5.7** (hierarchical scale priors).\n",
    "\n",
    "### Back-transformation to original units\n",
    "\\[\n",
    "m = \\frac{s_y}{s_x} m^{\\star},\\quad\n",
    "b = \\bar y + s_y\\!\\left(b^{\\star} - m^{\\star}\\frac{\\bar x}{s_x}\\right),\\quad\n",
    "o_i = s_y\\, o_i^{\\star},\\quad\n",
    "\\tau = s_y\\, \\tau^{\\star}.\n",
    "\\]\n",
    "\n",
    "### References for concepts\n",
    "- **Likelihood × Prior → Posterior**: *Bayes Rules!* **Ch. 2** (posterior \\(\\\\propto\\) prior × likelihood).  \n",
    "- **MCMC/HMC/NUTS approximation**: *Bayes Rules!* **Ch. 6**.  \n",
    "- **Exchangeability & hierarchical models**: **BDA3 Ch. 5.1–5.7** (e.g., Eight Schools).  \n",
    "- **Regression & hierarchical linear models**: **BDA3 Ch. 14** and **Ch. 15**.  \n",
    "- **Posterior predictive checks**: **BDA3 Ch. 6**.  \n",
    "- **Predictive performance & PSIS-LOO**: **BDA3 Ch. 7**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data loading and quick EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard scientific stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Load data (columns: lab, x, y, sigma)\n",
    "DATA_PATH = Path('/mnt/data/properMotions.csv')  # adjust path if needed\n",
    "df = pd.read_csv(DATA_PATH, header=None, names=['lab','x','y','sigma'])\n",
    "df['lab'] = df['lab'].astype(int)\n",
    "\n",
    "labs = np.sort(df['lab'].unique())\n",
    "n_labs = labs.size\n",
    "print(f\"N rows = {len(df)}, N labs = {n_labs}\")\n",
    "print(df.head())\n",
    "\n",
    "# Quick scatter: y vs x, colored by lab\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "for lab in labs:\n",
    "    d = df[df['lab']==lab]\n",
    "    ax.scatter(d['x'], d['y'], s=20, alpha=0.7, label=f\"lab {lab}\")\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_title(\"Observed data by lab\")\n",
    "ax.legend(ncols=3, fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Standardization (for sampler geometry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = df['x'].to_numpy().astype('float64')\n",
    "y = df['y'].to_numpy().astype('float64')\n",
    "sigma_obs = df['sigma'].to_numpy().astype('float64')\n",
    "lab = df['lab'].to_numpy().astype(int)\n",
    "\n",
    "x_mean, x_sd = x.mean(), x.std()\n",
    "y_mean, y_sd = y.mean(), y.std()\n",
    "\n",
    "x_star = (x - x_mean)/x_sd\n",
    "y_star = (y - y_mean)/y_sd\n",
    "sigma_star = sigma_obs / y_sd\n",
    "\n",
    "# Map lab IDs to 0..n_labs-1\n",
    "lab_to_idx = {lab_id:i for i, lab_id in enumerate(labs)}\n",
    "lab_idx = np.array([lab_to_idx[v] for v in lab], dtype=int)\n",
    "\n",
    "print(\"Standardized shapes:\", x_star.shape, y_star.shape, sigma_star.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Prior predictive simulation (to justify priors)\n",
    "\n",
    "Simulate from the prior to verify that the implied data scale is reasonable. (BDA3 **Ch. 6**; useful for checking priors.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "def prior_predictive(num_draws=1000):\n",
    "    m = rng.normal(0, 1, size=num_draws)\n",
    "    b = rng.normal(0, 1, size=num_draws)\n",
    "    tau = np.abs(rng.normal(0, 1, size=num_draws))  # Half-Normal(1)\n",
    "    # Sample lab offsets\n",
    "    o = rng.normal(0, tau[:, None], size=(num_draws, len(labs)))  # shape (draw, n_labs)\n",
    "    # Generate y* at observed x* with known sigma*\n",
    "    yrep = []\n",
    "    for d in range(num_draws):\n",
    "        mu = m[d]*x_star + b[d] + o[d, lab_idx]\n",
    "        yrep.append(rng.normal(mu, sigma_star))\n",
    "    return np.array(yrep)  # (draw, N)\n",
    "\n",
    "# Example quick check (commented for speed):\n",
    "# yrep = prior_predictive(200)\n",
    "# fig, ax = plt.subplots(figsize=(6,4))\n",
    "# ax.hist(yrep.ravel(), bins=40, density=True, alpha=0.7)\n",
    "# ax.set_title(\"Prior predictive (y*), Model B\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. TFP implementation (NUTS) — Models A and B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd, tfb, mcmc = tfp.distributions, tfp.bijectors, tfp.mcmc\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "x_tf = tf.convert_to_tensor(x_star, dtype=tf.float64)\n",
    "y_tf = tf.convert_to_tensor(y_star, dtype=tf.float64)\n",
    "sigma_tf = tf.convert_to_tensor(sigma_star, dtype=tf.float64)\n",
    "lab_idx_tf = tf.convert_to_tensor(lab_idx, dtype=tf.int32)\n",
    "n_labs_tf = tf.constant(int(lab_idx.max()+1), dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Model A — No offsets\n",
    "\\[\n",
    "y^{\\star}_n \\sim \\mathcal{N}(m^{\\star} x^{\\star}_n + b^{\\star},\\ \\sigma^{\\star}_n),\\quad\n",
    "m^{\\star}, b^{\\star} \\sim \\mathcal{N}(0,1).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def joint_no_offsets(x, sigma):\n",
    "    def model():\n",
    "        m = yield tfd.Normal(0., 1., name='m')\n",
    "        b = yield tfd.Normal(0., 1., name='b')\n",
    "        mu = m*x + b\n",
    "        y = yield tfd.Independent(tfd.Normal(mu, sigma), 1, name='y')\n",
    "    return tfd.JointDistributionCoroutineAutoBatched(model)\n",
    "\n",
    "jd_A = joint_no_offsets(x_tf, sigma_tf)\n",
    "jd_A_pinned = jd_A.experimental_pin(y=y_tf)\n",
    "\n",
    "def tlp_A(m, b):\n",
    "    return jd_A_pinned.log_prob(m, b)\n",
    "\n",
    "init_A = [tf.constant(0., tf.float64), tf.constant(0., tf.float64)]\n",
    "nuts_A = mcmc.NoUTurnSampler(target_log_prob_fn=tlp_A, step_size=0.2)\n",
    "nuts_A = mcmc.SimpleStepSizeAdaptation(nuts_A, num_adaptation_steps=1000)\n",
    "\n",
    "@tf.function(autograph=False, jit_compile=False)\n",
    "def run_A(num_draws=2000, num_burnin=2000):\n",
    "    return mcmc.sample_chain(num_results=num_draws,\n",
    "                             num_burnin_steps=num_burnin,\n",
    "                             current_state=init_A,\n",
    "                             kernel=nuts_A,\n",
    "                             trace_fn=lambda cs, kr: kr.inner_results.is_accepted)\n",
    "\n",
    "# Example (leave commented unless you want to run):\n",
    "# draws_A, acc_A = run_A(2000, 2000)\n",
    "# m_A, b_A = draws_A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Model B — Hierarchical offsets\n",
    "\\[\n",
    "\\\\begin{aligned}\n",
    "y^{\\\\star}_n &\\\\sim \\\\mathcal{N}(m^{\\\\star} x^{\\\\star}_n + b^{\\\\star} + o^{\\\\star}_{\\\\ell(n)},\\\\ \\\\sigma^{\\\\star}_n),\\\\\\\\\n",
    "o^{\\\\star}_i \\\\mid \\\\tau^{\\\\star} &\\\\sim \\\\mathcal{N}(0, \\\\tau^{\\\\star}),\\\\quad \\\\tau^{\\\\star}>0,\\\\\\\\\n",
    "m^{\\\\star}, b^{\\\\star} &\\\\sim \\\\mathcal{N}(0,1),\\\\quad \\\\tau^{\\\\star} \\\\sim \\\\text{Half-Normal}(1).\n",
    "\\\\end{aligned}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def joint_offsets(x, sigma, lab_idx, n_labs):\n",
    "    def model():\n",
    "        m = yield tfd.Normal(0., 1., name='m')\n",
    "        b = yield tfd.Normal(0., 1., name='b')\n",
    "        tau = yield tfd.HalfNormal(1., name='tau')\n",
    "        o = yield tfd.Sample(tfd.Normal(0., tau), sample_shape=[n_labs], name='o')\n",
    "        mu = m*x + b + tf.gather(o, lab_idx)\n",
    "        y = yield tfd.Independent(tfd.Normal(mu, sigma), 1, name='y')\n",
    "    return tfd.JointDistributionCoroutineAutoBatched(model)\n",
    "\n",
    "jd_B = joint_offsets(x_tf, sigma_tf, lab_idx_tf, n_labs_tf)\n",
    "jd_B_pinned = jd_B.experimental_pin(y=y_tf)\n",
    "\n",
    "def tlp_B(m, b, tau, o):\n",
    "    return jd_B_pinned.log_prob(m, b, tau, o)\n",
    "\n",
    "init_B = [tf.constant(0., tf.float64),\n",
    "          tf.constant(0., tf.float64),\n",
    "          tf.constant(0.5, tf.float64),\n",
    "          tf.zeros([int(n_labs_tf.numpy())], tf.float64)]\n",
    "\n",
    "bij_B = [tfb.Identity(), tfb.Identity(), tfb.Softplus(), tfb.Identity()]\n",
    "\n",
    "nuts_B = mcmc.NoUTurnSampler(target_log_prob_fn=tlp_B, step_size=0.2)\n",
    "nuts_B = mcmc.TransformedTransitionKernel(nuts_B, bijector=bij_B)\n",
    "nuts_B = mcmc.SimpleStepSizeAdaptation(nuts_B, num_adaptation_steps=1000)\n",
    "\n",
    "@tf.function(autograph=False, jit_compile=False)\n",
    "def run_B(num_draws=2000, num_burnin=2000):\n",
    "    return mcmc.sample_chain(num_results=num_draws,\n",
    "                             num_burnin_steps=num_burnin,\n",
    "                             current_state=init_B,\n",
    "                             kernel=nuts_B,\n",
    "                             trace_fn=lambda cs, kr: kr.inner_results.is_accepted)\n",
    "\n",
    "# Example (leave commented unless you want to run):\n",
    "# draws_B, acc_B = run_B(2000, 2000)\n",
    "# m_B, b_B, tau_B, o_B = draws_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 Back-transformation helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backtransform_mb(m_star, b_star, x_mean, x_sd, y_mean, y_sd):\n",
    "    m = (y_sd/x_sd)*m_star\n",
    "    b = y_mean + y_sd*(b_star - m_star*(x_mean/x_sd))\n",
    "    return m, b\n",
    "\n",
    "def backtransform_offsets(o_star, y_sd):\n",
    "    return o_star * y_sd\n",
    "\n",
    "def backtransform_tau(tau_star, y_sd):\n",
    "    return tau_star * y_sd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Model evaluation\n",
    "\n",
    "We use **posterior predictive checks** (graphical comparison of replicated vs. observed) and **PSIS-LOO** to compare Model A vs. Model B (BDA3 **Ch. 6–7**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Posterior predictive checks (PPC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Posterior predictive for Model B (as an example)\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def ppc_modelB(m_draws, b_draws, o_draws, n_show=50, seed=2024):\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     idx = rng.choice(m_draws.shape[0], size=min(n_show, m_draws.shape[0]), replace=False)\n",
    "#     fig, ax = plt.subplots(figsize=(6,4))\n",
    "#     ax.scatter(x_star, y_star, s=8, alpha=0.6, label='observed (stdzd)')\n",
    "#     for k in idx:\n",
    "#         mu = m_draws[k]*x_star + b_draws[k] + o_draws[k][lab_idx]\n",
    "#         yrep = rng.normal(mu, sigma_star)\n",
    "#         ax.plot(x_star, yrep, alpha=0.05)\n",
    "#     ax.set_title(\"PPC — Model B on standardized scale\"); ax.set_xlabel(\"x*\"); ax.set_ylabel(\"y*\")\n",
    "#     plt.show()\n",
    "#\n",
    "# ppc_modelB(m_B.numpy(), b_B.numpy(), o_B.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.2 PSIS-LOO model comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pip install arviz (if not available)\n",
    "# import arviz as az\n",
    "# import numpy as np\n",
    "#\n",
    "# def pointwise_ll_A(m_draws, b_draws):\n",
    "#     mu = np.einsum('d,n->dn', m_draws, x_star) + b_draws[:, None]\n",
    "#     return -0.5*np.log(2*np.pi*sigma_star**2) - 0.5*((y_star - mu)/sigma_star)**2\n",
    "#\n",
    "# def pointwise_ll_B(m_draws, b_draws, o_draws):\n",
    "#     mu = (np.einsum('d,n->dn', m_draws, x_star) + b_draws[:, None] +\n",
    "#           o_draws[:, lab_idx])\n",
    "#     return -0.5*np.log(2*np.pi*sigma_star**2) - 0.5*((y_star - mu)/sigma_star)**2\n",
    "#\n",
    "# ll_A = pointwise_ll_A(m_A.numpy(), b_A.numpy())\n",
    "# ll_B = pointwise_ll_B(m_B.numpy(), b_B.numpy(), o_B.numpy())\n",
    "#\n",
    "# id_A = az.from_dict(posterior={\"m\": m_A.numpy(), \"b\": b_A.numpy()},\n",
    "#                     log_likelihood={\"y\": ll_A})\n",
    "# id_B = az.from_dict(posterior={\"m\": m_B.numpy(), \"b\": b_B.numpy(), \"o\": o_B.numpy()},\n",
    "#                     log_likelihood={\"y\": ll_B})\n",
    "#\n",
    "# loo_A = az.loo(id_A, pointwise=True)\n",
    "# loo_B = az.loo(id_B, pointwise=True)\n",
    "# comp = az.compare({\"A_no_offsets\": id_A, \"B_offsets\": id_B}, ic=\"loo\", method=\"BB-pseudo-BMA\")\n",
    "# display(comp)\n",
    "#\n",
    "# print(\"\\nInterpretation tip: If ΔELPD (B − A) >> SE, prefer Model B (offsets).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Convergence diagnostics\n",
    "\n",
    "Use \\(\\hat{R}\\) and effective sample size (ESS); visualize traces (BDA3 **Ch. 10**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import arviz as az\n",
    "# id_B = az.from_dict(posterior={\"m\": m_B.numpy(), \"b\": b_B.numpy(), \"tau\": tau_B.numpy(), \"o\": o_B.numpy()})\n",
    "# summary = az.summary(id_B, var_names=['m','b','tau','o'])\n",
    "# display(summary)\n",
    "# az.plot_trace(id_B, var_names=['m','b','tau'])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Results (to fill after running)\n",
    "\n",
    "- **Slope \\(m\\)** and **intercept \\(b\\)** in original units (means and 90%/95% credible intervals).  \n",
    "- **Offsets \\(o_i\\)**: report posterior intervals; check which exclude 0.  \n",
    "- **Hyper-scale \\(\\tau\\)**: if concentrated near 0, offsets likely negligible; if away from 0, supports offsets.  \n",
    "- **Model comparison**: report PSIS-LOO \\(\\Delta\\)ELPD and SE; prefer the model with clearly higher ELPD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example transformation (after sampling):\n",
    "# m_B_orig, b_B_orig = backtransform_mb(m_B.numpy(), b_B.numpy(), x_mean, x_sd, y_mean, y_sd)\n",
    "# o_B_orig = backtransform_offsets(o_B.numpy(), y_sd)\n",
    "# tau_B_orig = backtransform_tau(tau_B.numpy(), y_sd)\n",
    "#\n",
    "# # Example plot of offsets in original units\n",
    "# def plot_offsets(o_draws_orig, labs, q=(0.05, 0.95)):\n",
    "#     lo, hi = np.quantile(o_draws_orig, q, axis=0)\n",
    "#     mu = o_draws_orig.mean(axis=0)\n",
    "#     ix = np.arange(len(labs))\n",
    "#     fig, ax = plt.subplots(figsize=(6,4))\n",
    "#     ax.errorbar(ix, mu, yerr=[mu-lo, hi-mu], fmt='o', capsize=3)\n",
    "#     ax.axhline(0, color='k', lw=1, ls='--')\n",
    "#     ax.set_xticks(ix); ax.set_xticklabels([str(l) for l in labs])\n",
    "#     ax.set_xlabel(\"lab\"); ax.set_ylabel(\"offset $o_i$ (original units)\"); ax.set_title(\"Posterior offsets (90% CI)\")\n",
    "#     plt.tight_layout(); plt.show()\n",
    "#\n",
    "# plot_offsets(o_B_orig, labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Brief summary (fill in after you run)\n",
    "- Is there **evidence for experiment-dependent zero-point offsets**? (Use \\(\\tau\\), \\(o_i\\) intervals, and PSIS-LOO.)  \n",
    "- Final linear relationship \\(y \\approx m x + b\\) (original units) with credible intervals.  \n",
    "- Any labs with notably non-zero offsets?  \n",
    "- Diagnostics and model-check highlights (e.g., PPC fit, Pareto-\\(k\\) warnings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "- **BDA3 — Bayesian Data Analysis (3rd ed.)**\n",
    "  - **Ch. 5.1–5.7**: Exchangeability & hierarchical models (e.g., Eight Schools).  \n",
    "  - **Ch. 6**: Model checking & posterior predictive checks.  \n",
    "  - **Ch. 7**: Predictive performance & PSIS-LOO.  \n",
    "  - **Ch. 10**: Computation & diagnostics (\\(\\hat{R}\\), ESS).  \n",
    "  - **Ch. 14–15**: Regression & hierarchical linear models.\n",
    "- **Bayes Rules!**\n",
    "  - **Ch. 2**: Prior × likelihood \\(\\Rightarrow\\) posterior.  \n",
    "  - **Ch. 6**: MCMC for posterior approximation.  \n",
    "  - **Ch. 15**: Hierarchical models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}